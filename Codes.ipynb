{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test Run.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ZtldQcOZCZdi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ]
    },
    {
      "metadata": {
        "id": "I9oChNMdqEPw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import math\n",
        "from datetime import datetime\n",
        "import time\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZd-YLLDCche",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set directories"
      ]
    },
    {
      "metadata": {
        "id": "YArxH54wqi31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = '/tmp/cifar10_data'\n",
        "train_dir = '/tmp/cifar10_train'\n",
        "eval_dir = '/tmp/cifar10_eval'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuzsPdXsCgQa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set scalars"
      ]
    },
    {
      "metadata": {
        "id": "4z1k4Hrt2ex3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_size = 50000\n",
        "test_size = 10000\n",
        "batch_size = 128\n",
        "\n",
        "new_size = 24\n",
        "\n",
        "num_label = 10\n",
        "\n",
        "max_step_train = 20000\n",
        "\n",
        "max_step_eval = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mahtroWJC4UQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data link to download from"
      ]
    },
    {
      "metadata": {
        "id": "E5Z07pJ3qYOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data link\n",
        "DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2pFdDCgsur3A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download and extract data"
      ]
    },
    {
      "metadata": {
        "id": "sf-edH_NqeUk",
        "colab_type": "code",
        "outputId": "4b37a5b3-0047-4eba-9efc-b623e8e0a7cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists(data_dir):\n",
        "   os.makedirs(data_dir)  \n",
        "filename = DATA_URL.split('/')[-1]\n",
        "filepath = os.path.join(data_dir, filename)           \n",
        "if not os.path.exists(filepath):\n",
        "  def rh(count, block_size, total_size):\n",
        "      sys.stdout.write('\\r>> Downloading %s ' % filename)\n",
        "      sys.stdout.flush()  \n",
        "  filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, rh)\n",
        "  print('\\r>> Successfully downloaded', filename)\n",
        "extracted_path = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
        "if not os.path.exists(extracted_path):\n",
        "  tarfile.open(filepath, 'r:gz').extractall(data_dir)\n",
        "print('file name:', filename)\n",
        "print('file path:', filepath)\n",
        "print('extracted path:', extracted_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file name: cifar-10-binary.tar.gz\n",
            "file path: /tmp/cifar10_data/cifar-10-binary.tar.gz\n",
            "extracted path: /tmp/cifar10_data/cifar-10-batches-bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Lzf-IINtC_UA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Upload and decode cifar10"
      ]
    },
    {
      "metadata": {
        "id": "v9hqBppTvIUU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_cifar10(filename_queue):\n",
        "#Original image dimensions\n",
        "    original_height = 32\n",
        "    original_width = 32\n",
        "    original_depth = 3\n",
        "\n",
        "#Original Bytes\n",
        "    label_bytes = 1 \n",
        "    image_bytes = original_height * original_width * original_depth\n",
        "    record_bytes = label_bytes + image_bytes\n",
        "\n",
        "\n",
        "    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n",
        "    key,value = reader.read(filename_queue)\n",
        "\n",
        "    value_pixel = tf.decode_raw(value, tf.uint8)\n",
        "\n",
        "    value_label = tf.cast(tf.strided_slice(value_pixel, [0], [label_bytes]), tf.int32)\n",
        "\n",
        "    depth_major = tf.reshape(tf.strided_slice(value_pixel, \n",
        "                                          [label_bytes], \n",
        "                                          [label_bytes + image_bytes]),\n",
        "                         [original_depth, original_height, original_width])\n",
        "\n",
        "    image_unit8 = tf.transpose(depth_major, [1, 2, 0])\n",
        "    \n",
        "    return image_unit8, value_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3K-o8dQGQCgk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "metadata": {
        "id": "Q-bwpyBfQBax",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def distort_data(filename_queue):\n",
        "  with tf.name_scope('preprocessing'):\n",
        "    image_decoded, label_decoded = decode_cifar10(filename_queue)\n",
        "    reshaped_image = tf.cast(image_decoded, tf.float32)\n",
        "    # Convert image data from unit8 to float32\n",
        "\n",
        "    # Trucated images\n",
        "    height = 24\n",
        "    width = 24\n",
        "    distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n",
        " \n",
        "    # Randomly distort image\n",
        "    distorted_image = tf.image.random_flip_left_right(distorted_image) # horizontal\n",
        "    distorted_image = tf.image.random_brightness(distorted_image,\n",
        "                                                   max_delta=63)  # brightness\n",
        "    distorted_image = tf.image.random_contrast(distorted_image,\n",
        "                                                 lower=0.2, \n",
        "                                                 upper=1.8) # contrast\n",
        "    # Standardization\n",
        "    float_image = tf.image.per_image_standardization(distorted_image)\n",
        "\n",
        "    float_image.set_shape([height, width, 3])\n",
        "      \n",
        "    label_decoded.set_shape([1])\n",
        "\n",
        "    batch_size = 128\n",
        "    \n",
        "    # To make sure the shuttfle process is substantial\n",
        "    min_fraction_of_examples_in_queue = 0.4\n",
        "    min_queue_examples = int(train_size*min_fraction_of_examples_in_queue)\n",
        "    batch_pixel, batch_label = tf.train.shuffle_batch([float_image, label_decoded],\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   num_threads=16,\n",
        "                                                   capacity=min_queue_examples + 3 * batch_size,\n",
        "                                                   min_after_dequeue=min_queue_examples)\n",
        "    batch_label = tf.reshape(batch_label, [batch_size])\n",
        "  return batch_pixel, batch_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOWEr2_EQSuO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load data for training"
      ]
    },
    {
      "metadata": {
        "id": "GDhIxUu9QSDz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_data():\n",
        "  if not data_dir:\n",
        "    raise ValueError('Please supply a data_dir')\n",
        "  new_data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
        "  \n",
        "  filenames = [os.path.join(new_data_dir, 'data_batch_%d.bin' % i)\n",
        "               for i in range(1, 6)] \n",
        "  for f in filenames:\n",
        "    if not tf.gfile.Exists(f):\n",
        "      raise ValueError('Failed to find file: ' + f)\n",
        "      \n",
        "  filename_queue = tf.train.string_input_producer(filenames)\n",
        "  \n",
        "  distorted_image, label = distort_data(filename_queue)\n",
        "  \n",
        "  return distorted_image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t_8XybJlDcZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define model: Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "PWZ97IOGB_Ht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build logistic regression model\n",
        "def inference_lg(images):\n",
        "  # images: returned from distorted_inputs_model\n",
        "  \n",
        "  # logits\n",
        "  with tf.variable_scope('logits') as scope:\n",
        "    reshape = tf.reshape(images, [images.get_shape().as_list()[0], -1])\n",
        "    dim = reshape.get_shape()[1].value\n",
        "    weights = _variable_with_weight_decay('weights', \n",
        "                                          shape = [dim, num_label],\n",
        "                                          stddev=0.04, wd=0.04)\n",
        "    biases = tf.get_variable('biases', [num_label], dtype=tf.float32,\n",
        "                             initializer=tf.constant_initializer(0.0))\n",
        "    softmax_linear = tf.add(tf.matmul(reshape, weights), biases, name=scope.name)\n",
        "    _activation_summary(softmax_linear)\n",
        "\n",
        "  return softmax_linear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T9buHNOC8lt-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define model: CNN"
      ]
    },
    {
      "metadata": {
        "id": "3KKITMmn8mB_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build CNN model with Max-Pooling\n",
        "def inference_cnn(images):\n",
        "  \n",
        "  # convolutional 1: \n",
        "  with tf.variable_scope('conv1') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[5, 5, 3, 64],\n",
        "                                         stddev=5e-2,\n",
        "                                         wd=None)   \n",
        "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = tf.get_variable('biases', [64], \n",
        "                             tf.float32,\n",
        "                             tf.constant_initializer(0.0))\n",
        "    pre_activation = tf.nn.bias_add(conv, biases)\n",
        "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
        "    _activation_summary(conv1)\n",
        "    \n",
        "  # pooling 1\n",
        "  ## max pooling:\n",
        "  pool1 = tf.nn.max_pool(conv1, \n",
        "                         ksize=[1, 2, 2, 1], \n",
        "                         strides=[1, 2, 2, 1],\n",
        "                         padding='SAME',\n",
        "                         name='pool1')    \n",
        "  \n",
        "  ## fractional max pooling 1:\n",
        "  #pool1 = tf.nn.fractional_max_pool(conv1,\n",
        "  #                                  pooling_ratio=[1.0, 1.44, 1.44, 1.0],\n",
        "  #                                  name = 'pool1')\n",
        "   \n",
        "  ## average pooling 1\n",
        "  #pool1 = tf.nn.avg_pool(conv1,\n",
        "  #                      ksize=[1,2,2,1],\n",
        "  #                      strides=[1,2,2,1],\n",
        "  #                      padding='SAME',\n",
        "  #                      name='pool1')\n",
        "  \n",
        "  \n",
        "  # batch normalization 1\n",
        "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
        "                    name='norm1')\n",
        "\n",
        "  # convolutional 2: \n",
        "  with tf.variable_scope('conv2') as scope:\n",
        "    kernel = _variable_with_weight_decay('weights',\n",
        "                                         shape=[5, 5, 64, 64],\n",
        "                                         stddev=5e-2,\n",
        "                                         wd=None)\n",
        "    conv2 = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "    biases = tf.get_variable('biases', [64], \n",
        "                              tf.float32,\n",
        "                              tf.constant_initializer(0.1))\n",
        "    pre_activation = tf.nn.bias_add(conv, biases)\n",
        "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
        "    _activation_summary(conv2)\n",
        "\n",
        "  # batch normalization 2\n",
        "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
        "                    name='norm2')\n",
        "  # pooling 2:\n",
        "  ## max pooling 2:\n",
        "  #pool2 = tf.nn.max_pool(norm2, \n",
        "  #                       ksize=[1, 2, 2, 1],\n",
        "  #                       strides=[1, 2, 2, 1], \n",
        "  #                       padding='SAME',\n",
        "  #                       name='pool2')\n",
        "  \n",
        "  #fractional max pooling: \n",
        "  pool2 = tf.nn.fractional_max_pool(norm2,\n",
        "                                    pooling_ratio=[1.0, 1.44, 1.44, 1.0],\n",
        "                                    name = 'pool1')\n",
        "  \n",
        "  ## average pooling 2\n",
        "  #pool2 = tf.nn.avg_pool(conv2,\n",
        "  #                      ksize=[1,2,2,1],\n",
        "  #                      strides=[1,2,2,1],\n",
        "  #                      name='pool2')\n",
        "  \n",
        "  # fully connected 3\n",
        "  with tf.variable_scope('local3') as scope:\n",
        "    reshape = tf.reshape(pool2.output, [images.get_shape().as_list()[0], -1])\n",
        "    dim = reshape.get_shape()[1].value\n",
        "    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
        "                                          stddev=0.04, wd=0.004)\n",
        "    biases = tf.get_variable('biases', [384], \n",
        "                             tf.float32,\n",
        "                             tf.constant_initializer(0.1))\n",
        "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
        "    _activation_summary(local3)\n",
        "\n",
        "  # fully connected 4\n",
        "  with tf.variable_scope('local4') as scope:\n",
        "    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
        "                                          stddev=0.04, wd=0.004)\n",
        "    biases = tf.get_variable('biases', [192], \n",
        "                              tf.float32,\n",
        "                              tf.constant_initializer(0.1))\n",
        "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
        "    _activation_summary(local4)\n",
        "\n",
        "  # fully connected to logit\n",
        "  with tf.variable_scope('softmax_linear') as scope:\n",
        "    weights = _variable_with_weight_decay('weights', [192, num_label],\n",
        "                                          stddev=1/192.0, wd=None)\n",
        "    biases = tf.get_variable('biases', num_label,\n",
        "                              tf.float32,\n",
        "                              tf.constant_initializer(0.0))\n",
        "    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
        "    _activation_summary(softmax_linear)\n",
        "\n",
        "  return softmax_linear\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KFWE0p0iDpw3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define loss"
      ]
    },
    {
      "metadata": {
        "id": "LwyZrCNUCCW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_loss(logits, labels):\n",
        "  #logist and labels are returned from inference function\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "      labels=labels, logits=logits, name='cross_entropy_per_example')\n",
        "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "  tf.add_to_collection('losses', cross_entropy_mean)\n",
        "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfC0OHaPCEDm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _add_loss_summaries(total_loss):\n",
        "  #total_loss is returned from get_loss\n",
        "  # Compute the moving average of all individual losses and the total loss.\n",
        "  loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
        "  losses = tf.get_collection('losses')\n",
        "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
        "\n",
        "  # Attach a scalar summary to all individual losses and the total loss\n",
        "  for l in losses + [total_loss]:\n",
        "    # Name each loss as '(raw)' and name the moving average version of the loss\n",
        "    # as the original loss name.\n",
        "    tf.summary.scalar(l.op.name + ' (raw)', l)\n",
        "    tf.summary.scalar(l.op.name, loss_averages.average(l))\n",
        "\n",
        "  return loss_averages_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o7tBT8NDDWKg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialization of variables"
      ]
    },
    {
      "metadata": {
        "id": "E24e2G-ZBzpw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
        "    # name: name of the variable\n",
        "    # shape: list of ints indicatin dimension of parameters\n",
        "    # stddev: standard deviation of a truncated Gaussian\n",
        "    # wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
        "    #    decay is not added for this Variable\n",
        "  init = tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32)  \n",
        "  var = tf.get_variable(name, shape, initializer=init, dtype = tf.float32)\n",
        "  if wd is not None:\n",
        "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
        "    tf.add_to_collection('losses', weight_decay)\n",
        "  return var"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G0-W-Vr5D7b-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "hYi7m3xmCRPq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimizer(total_loss, global_step):\n",
        "    # total_loss: returned from get_loss().\n",
        "    # global_step: Integer Variable counting the number of training steps processed.\n",
        "\n",
        "  # variables that affect learning rate\n",
        "  num_batches_per_epoch = train_size / batch_size\n",
        "  decay_steps = int(num_batches_per_epoch * 350)\n",
        "\n",
        "  # decay the learning rate exponentially based on the number of steps.\n",
        "  # decayed_learning_rate = learning_rate *decay_rate ^ (global_step / decay_steps)\n",
        "  lr = tf.train.exponential_decay(0.1,\n",
        "                                  global_step,\n",
        "                                  decay_steps,\n",
        "                                  0.1,\n",
        "                                  staircase=True)\n",
        "  tf.summary.scalar('learning_rate', lr)\n",
        "\n",
        "  # generate moving averages of all losses and associated summaries\n",
        "  loss_averages_op = _add_loss_summaries(total_loss)\n",
        "\n",
        "  # compute gradients.\n",
        "  with tf.control_dependencies([total_loss]):\n",
        "       opt = tf.train.GradientDescentOptimizer(lr)\n",
        "       grads = opt.compute_gradients(total_loss)\n",
        "\n",
        "  # apply gradients\n",
        "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
        "\n",
        "  # add histograms for trainable variables.\n",
        "  for var in tf.trainable_variables():\n",
        "    tf.summary.histogram(var.op.name, var)\n",
        "\n",
        "  # add histograms for gradients.\n",
        "  for grad, var in grads:\n",
        "    if grad is not None:\n",
        "      tf.summary.histogram(var.op.name + '/gradients', grad)\n",
        "\n",
        "  # track the moving averages of all trainable variables.\n",
        "  variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)\n",
        "  with tf.control_dependencies([apply_gradient_op]):\n",
        "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
        "\n",
        "  return variables_averages_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9IJmoRLbD9_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ]
    },
    {
      "metadata": {
        "id": "qZfQVtirDSqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model_type):\n",
        "  with tf.Graph().as_default():\n",
        "    global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "    # get images and labels for CIFAR-10.\n",
        "    # force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
        "    # GPU and resulting in a slow down.\n",
        "    images_train, labels_train = train_data()\n",
        "\n",
        "    # computes the logits predictions from the\n",
        "    logits_train = model_type(images_train)\n",
        "    \n",
        "    # train loss.\n",
        "    loss_train = get_loss(logits_train, labels_train)\n",
        "    tf.summary.scalar('Loss', loss_train)\n",
        "\n",
        "    \n",
        "    train_op = optimizer(loss_train, global_step)\n",
        "\n",
        "    class _LoggerHook(tf.train.SessionRunHook):\n",
        "      \"\"\"Logs loss and runtime.\"\"\"\n",
        "\n",
        "      def begin(self):\n",
        "        self._step = -1\n",
        "        self._start_time = time.time()\n",
        "\n",
        "      def before_run(self, run_context):\n",
        "        self._step += 1\n",
        "        return tf.train.SessionRunArgs(loss_train)  # Asks for loss value.\n",
        "\n",
        "      def after_run(self, run_context, run_values):\n",
        "        freq = 100\n",
        "        if self._step % freq == 0:\n",
        "          current_time = time.time()\n",
        "          duration = current_time - self._start_time\n",
        "          self._start_time = current_time\n",
        "\n",
        "          loss_value = run_values.results\n",
        "          examples_per_sec = freq * batch_size / duration\n",
        "          sec_per_batch = float(duration / freq)\n",
        "\n",
        "          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
        "                        'sec/batch)')\n",
        "          print (format_str % (datetime.now(), self._step, loss_value,\n",
        "                               examples_per_sec, sec_per_batch))\n",
        "\n",
        "    with tf.train.MonitoredTrainingSession(checkpoint_dir=train_dir,\n",
        "               hooks=[tf.train.StopAtStepHook(last_step=max_step_train),\n",
        "                      tf.train.NanTensorHook(loss_train),\n",
        "                       _LoggerHook()],\n",
        "        config=tf.ConfigProto(log_device_placement=False)) as mon_sess:\n",
        "      while not mon_sess.should_stop():\n",
        "        mon_sess.run(train_op)\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EISyV6MCDHzO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define summary for tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "uhVBFh-VBwVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _activation_summary(x):\n",
        "  #x: Tensor\n",
        "  tensor_name =  x.op.name\n",
        "  tf.summary.histogram(tensor_name + '/activations', x)\n",
        "  tf.summary.scalar(tensor_name + '/sparsity',\n",
        "                                       tf.nn.zero_fraction(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6LPfvSeECJ_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Execution of training"
      ]
    },
    {
      "metadata": {
        "id": "JTPcmv1NEGVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if tf.gfile.Exists(train_dir):\n",
        "   tf.gfile.DeleteRecursively(train_dir)\n",
        "   tf.gfile.MakeDirs(train_dir)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mc9Bt70ANRRN",
        "colab_type": "code",
        "outputId": "0b4670ef-2d85-4639-e4e8-80adf0c65ab0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3975
        }
      },
      "cell_type": "code",
      "source": [
        "train(inference_cnn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n",
            "INFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n",
            "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n",
            "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/cifar10_train/model.ckpt.\n",
            "2018-12-17 07:24:12.846361: step 0, loss = 18.07 (472.2 examples/sec; 0.271 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.0595\n",
            "2018-12-17 07:24:37.514020: step 100, loss = 16.49 (518.9 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.98817\n",
            "2018-12-17 07:25:02.565081: step 200, loss = 15.15 (511.0 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01472\n",
            "2018-12-17 07:25:27.481514: step 300, loss = 14.05 (513.7 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.96781\n",
            "2018-12-17 07:25:52.673682: step 400, loss = 13.18 (508.1 examples/sec; 0.252 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05558\n",
            "2018-12-17 07:26:17.343130: step 500, loss = 12.29 (518.9 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01632\n",
            "2018-12-17 07:26:42.238687: step 600, loss = 11.35 (514.1 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.94338\n",
            "2018-12-17 07:27:07.587733: step 700, loss = 10.63 (505.0 examples/sec; 0.253 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02712\n",
            "2018-12-17 07:27:32.438236: step 800, loss = 10.01 (515.1 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04479\n",
            "2018-12-17 07:27:57.147815: step 900, loss = 9.18 (518.0 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.98249\n",
            "2018-12-17 07:28:22.279279: step 1000, loss = 8.59 (509.3 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.06255\n",
            "2018-12-17 07:28:46.887867: step 1100, loss = 8.22 (520.1 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.96574\n",
            "2018-12-17 07:29:12.078885: step 1200, loss = 7.70 (508.1 examples/sec; 0.252 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.0188\n",
            "2018-12-17 07:29:36.962010: step 1300, loss = 7.06 (514.4 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04021\n",
            "2018-12-17 07:30:01.740004: step 1400, loss = 6.45 (516.6 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.97698\n",
            "2018-12-17 07:30:26.864091: step 1500, loss = 6.21 (509.5 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.98746\n",
            "2018-12-17 07:30:51.950668: step 1600, loss = 5.79 (510.2 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.0518\n",
            "2018-12-17 07:31:16.643349: step 1700, loss = 5.53 (518.4 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.98331\n",
            "2018-12-17 07:31:41.746161: step 1800, loss = 5.19 (509.9 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05294\n",
            "2018-12-17 07:32:06.412285: step 1900, loss = 5.05 (518.9 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.95111\n",
            "2018-12-17 07:32:31.720954: step 2000, loss = 4.48 (505.8 examples/sec; 0.253 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00912\n",
            "2018-12-17 07:32:56.647464: step 2100, loss = 4.45 (513.5 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.10861\n",
            "2018-12-17 07:33:21.005655: step 2200, loss = 4.06 (525.5 examples/sec; 0.244 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.93385\n",
            "2018-12-17 07:33:46.409754: step 2300, loss = 3.88 (503.9 examples/sec; 0.254 sec/batch)\n",
            "INFO:tensorflow:Saving checkpoints for 2304 into /tmp/cifar10_train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 4.00717\n",
            "2018-12-17 07:34:11.368619: step 2400, loss = 3.79 (512.8 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05092\n",
            "2018-12-17 07:34:36.046966: step 2500, loss = 3.52 (518.7 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01183\n",
            "2018-12-17 07:35:00.980620: step 2600, loss = 3.31 (513.4 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02521\n",
            "2018-12-17 07:35:25.823220: step 2700, loss = 3.17 (515.2 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02951\n",
            "2018-12-17 07:35:50.642825: step 2800, loss = 3.13 (515.7 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02863\n",
            "2018-12-17 07:36:15.485705: step 2900, loss = 2.80 (515.2 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00287\n",
            "2018-12-17 07:36:40.438239: step 3000, loss = 2.72 (513.0 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00805\n",
            "2018-12-17 07:37:05.392379: step 3100, loss = 2.86 (512.9 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01574\n",
            "2018-12-17 07:37:30.296745: step 3200, loss = 2.55 (514.0 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.98803\n",
            "2018-12-17 07:37:55.369366: step 3300, loss = 2.53 (510.5 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01799\n",
            "2018-12-17 07:38:20.255651: step 3400, loss = 2.33 (514.3 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.08039\n",
            "2018-12-17 07:38:44.767103: step 3500, loss = 2.33 (522.2 examples/sec; 0.245 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.98812\n",
            "2018-12-17 07:39:09.860268: step 3600, loss = 2.24 (510.1 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.97884\n",
            "2018-12-17 07:39:34.982220: step 3700, loss = 2.29 (509.5 examples/sec; 0.251 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.03809\n",
            "2018-12-17 07:39:59.734333: step 3800, loss = 2.05 (517.1 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02526\n",
            "2018-12-17 07:40:24.580405: step 3900, loss = 1.98 (515.2 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.03718\n",
            "2018-12-17 07:40:49.353150: step 4000, loss = 2.00 (516.7 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00298\n",
            "2018-12-17 07:41:14.352099: step 4100, loss = 1.75 (512.0 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01743\n",
            "2018-12-17 07:41:39.222497: step 4200, loss = 1.81 (514.7 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02912\n",
            "2018-12-17 07:42:04.040084: step 4300, loss = 1.94 (515.8 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00984\n",
            "2018-12-17 07:42:28.979949: step 4400, loss = 1.87 (513.2 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02794\n",
            "2018-12-17 07:42:53.808651: step 4500, loss = 1.83 (515.5 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.03801\n",
            "2018-12-17 07:43:18.568687: step 4600, loss = 1.54 (517.0 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04239\n",
            "2018-12-17 07:43:43.320473: step 4700, loss = 1.52 (517.1 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:Saving checkpoints for 4718 into /tmp/cifar10_train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 4.0057\n",
            "2018-12-17 07:44:08.269843: step 4800, loss = 1.63 (513.0 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05456\n",
            "2018-12-17 07:44:32.933653: step 4900, loss = 1.67 (519.0 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05215\n",
            "2018-12-17 07:44:57.618491: step 5000, loss = 1.76 (518.5 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05943\n",
            "2018-12-17 07:45:22.263902: step 5100, loss = 1.65 (519.4 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.0516\n",
            "2018-12-17 07:45:46.946788: step 5200, loss = 1.40 (518.6 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01081\n",
            "2018-12-17 07:46:11.885392: step 5300, loss = 1.39 (513.3 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.94662\n",
            "2018-12-17 07:46:37.227615: step 5400, loss = 1.46 (505.1 examples/sec; 0.253 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.06718\n",
            "2018-12-17 07:47:01.792901: step 5500, loss = 1.48 (521.1 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.97508\n",
            "2018-12-17 07:47:26.943233: step 5600, loss = 1.50 (508.9 examples/sec; 0.252 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.99545\n",
            "2018-12-17 07:47:51.969976: step 5700, loss = 1.31 (511.5 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01456\n",
            "2018-12-17 07:48:16.885337: step 5800, loss = 1.58 (513.7 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.07225\n",
            "2018-12-17 07:48:41.459330: step 5900, loss = 1.41 (520.9 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00943\n",
            "2018-12-17 07:49:06.381306: step 6000, loss = 1.29 (513.6 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00473\n",
            "2018-12-17 07:49:31.363230: step 6100, loss = 1.13 (512.4 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02137\n",
            "2018-12-17 07:49:56.214942: step 6200, loss = 1.44 (515.1 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.11164\n",
            "2018-12-17 07:50:20.562214: step 6300, loss = 1.31 (525.7 examples/sec; 0.243 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.9956\n",
            "2018-12-17 07:50:45.581823: step 6400, loss = 1.19 (511.6 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.9547\n",
            "2018-12-17 07:51:10.852583: step 6500, loss = 1.48 (506.5 examples/sec; 0.253 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05267\n",
            "2018-12-17 07:51:35.548320: step 6600, loss = 1.33 (518.3 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00648\n",
            "2018-12-17 07:52:00.491371: step 6700, loss = 1.35 (513.2 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04254\n",
            "2018-12-17 07:52:25.229297: step 6800, loss = 1.40 (517.4 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00436\n",
            "2018-12-17 07:52:50.199821: step 6900, loss = 1.32 (512.6 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05996\n",
            "2018-12-17 07:53:14.837036: step 7000, loss = 1.43 (519.5 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05338\n",
            "2018-12-17 07:53:39.500393: step 7100, loss = 1.38 (519.0 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:Saving checkpoints for 7133 into /tmp/cifar10_train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 3.91102\n",
            "2018-12-17 07:54:05.068518: step 7200, loss = 1.39 (500.6 examples/sec; 0.256 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05337\n",
            "2018-12-17 07:54:29.745078: step 7300, loss = 1.26 (518.7 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.0662\n",
            "2018-12-17 07:54:54.352334: step 7400, loss = 1.39 (520.2 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00809\n",
            "2018-12-17 07:55:19.283432: step 7500, loss = 1.28 (513.4 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.09562\n",
            "2018-12-17 07:55:43.709936: step 7600, loss = 1.22 (524.0 examples/sec; 0.244 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01127\n",
            "2018-12-17 07:56:08.637459: step 7700, loss = 1.39 (513.5 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.07025\n",
            "2018-12-17 07:56:33.211261: step 7800, loss = 1.35 (520.9 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00148\n",
            "2018-12-17 07:56:58.191424: step 7900, loss = 1.29 (512.4 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.09363\n",
            "2018-12-17 07:57:22.632547: step 8000, loss = 1.36 (523.7 examples/sec; 0.244 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.9928\n",
            "2018-12-17 07:57:47.659315: step 8100, loss = 1.15 (511.5 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01745\n",
            "2018-12-17 07:58:12.548029: step 8200, loss = 1.27 (514.3 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04779\n",
            "2018-12-17 07:58:37.284188: step 8300, loss = 1.47 (517.5 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02071\n",
            "2018-12-17 07:59:02.133286: step 8400, loss = 1.42 (515.1 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.03715\n",
            "2018-12-17 07:59:26.900536: step 8500, loss = 1.37 (516.8 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.05375\n",
            "2018-12-17 07:59:51.569939: step 8600, loss = 1.34 (518.9 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00038\n",
            "2018-12-17 08:00:16.564886: step 8700, loss = 1.01 (512.1 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.07711\n",
            "2018-12-17 08:00:41.093764: step 8800, loss = 1.28 (521.8 examples/sec; 0.245 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.0258\n",
            "2018-12-17 08:01:05.934991: step 8900, loss = 1.24 (515.3 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.07048\n",
            "2018-12-17 08:01:30.512938: step 9000, loss = 1.39 (520.8 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.94709\n",
            "2018-12-17 08:01:55.830041: step 9100, loss = 1.22 (505.6 examples/sec; 0.253 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.08377\n",
            "2018-12-17 08:02:20.334490: step 9200, loss = 1.24 (522.4 examples/sec; 0.245 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.03178\n",
            "2018-12-17 08:02:45.127514: step 9300, loss = 1.30 (516.3 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.01765\n",
            "2018-12-17 08:03:10.011180: step 9400, loss = 1.14 (514.4 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.07561\n",
            "2018-12-17 08:03:34.560364: step 9500, loss = 1.13 (521.4 examples/sec; 0.245 sec/batch)\n",
            "INFO:tensorflow:Saving checkpoints for 9556 into /tmp/cifar10_train/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 3.996\n",
            "2018-12-17 08:03:59.575296: step 9600, loss = 1.27 (511.7 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00006\n",
            "2018-12-17 08:04:24.569962: step 9700, loss = 1.25 (512.1 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02847\n",
            "2018-12-17 08:04:49.398812: step 9800, loss = 1.03 (515.5 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.02008\n",
            "2018-12-17 08:05:14.269830: step 9900, loss = 1.10 (514.7 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.06421\n",
            "2018-12-17 08:05:38.891083: step 10000, loss = 1.20 (519.9 examples/sec; 0.246 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04151\n",
            "2018-12-17 08:06:03.641095: step 10100, loss = 1.28 (517.2 examples/sec; 0.248 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04246\n",
            "2018-12-17 08:06:28.378043: step 10200, loss = 1.21 (517.4 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.00729\n",
            "2018-12-17 08:06:53.332695: step 10300, loss = 1.42 (512.9 examples/sec; 0.250 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.04321\n",
            "2018-12-17 08:07:18.059842: step 10400, loss = 1.32 (517.6 examples/sec; 0.247 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 3.94742\n",
            "2018-12-17 08:07:43.383637: step 10500, loss = 1.23 (505.5 examples/sec; 0.253 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.0101\n",
            "2018-12-17 08:08:08.318265: step 10600, loss = 1.19 (513.3 examples/sec; 0.249 sec/batch)\n",
            "INFO:tensorflow:global_step/sec: 4.06012\n",
            "2018-12-17 08:08:32.959112: step 10700, loss = 1.23 (519.5 examples/sec; 0.246 sec/batch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J-4p9VSuGfgO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "qZNsgUsOB-_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training\n",
        "LOG_DIR = train_dir\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "# Download and unzip ngrok - you will only need to do this once per session\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Launch the ngrok background process\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# Get the public URL and be sorted!\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "meq8Ka3UGfng",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "23leC7VeRnRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data for evaluation\n",
        "def eval_data():\n",
        "  if not data_dir:\n",
        "    raise ValueError('Please supply a data_dir')\n",
        "  new_data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
        "  \n",
        "  filenames = [os.path.join(new_data_dir, 'test_batch.bin')]\n",
        "  for f in filenames:\n",
        "      if not tf.gfile.Exists(f):\n",
        "        raise ValueError('Failed to find file: ' + f)\n",
        "        \n",
        "  filename_queue = tf.train.string_input_producer(filenames)\n",
        "  \n",
        "  distorted_image, label = distort_data(filename_queue)\n",
        "  \n",
        "  return distorted_image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g6TVgZeeYQ0S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#evaluation on the whole set\n",
        "def evaluate(saver, summary_writer, top_k_op, summary_op):\n",
        "  with tf.Session() as sess:\n",
        "    ckpt = tf.train.get_checkpoint_state(train_dir)\n",
        "    if ckpt and ckpt.model_checkpoint_path:\n",
        "      saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "      global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
        "    else:\n",
        "      print('No checkpoint file found')\n",
        "      return\n",
        "\n",
        "    # Start the queue runners.\n",
        "    coord = tf.train.Coordinator()\n",
        "    try:\n",
        "      threads = []\n",
        "      for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n",
        "        threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n",
        "                                         start=True))\n",
        "\n",
        "      num_iter = int(math.ceil(test_size / batch_size))\n",
        "      true_count = 0  # Counts the number of correct predictions.\n",
        "      total_sample_count = num_iter * batch_size\n",
        "      \n",
        "      step = 0\n",
        "      while step < num_iter and not coord.should_stop():\n",
        "         predictions = sess.run([top_k_op])\n",
        "         true_count += np.sum(predictions)\n",
        "         step += 1\n",
        "\n",
        "      # Compute precision @ 1.\n",
        "      precision = true_count / total_sample_count\n",
        "      print('%s: precision @ 1 = %.3f' % (datetime.now(), precision))\n",
        "\n",
        "      summary = tf.Summary()\n",
        "      summary.ParseFromString(sess.run(summary_op))\n",
        "      summary.value.add(tag='Precision @ 1', simple_value=precision)\n",
        "      summary_writer.add_summary(summary, global_step)\n",
        "    except Exception as e:  # pylint: disable=broad-except\n",
        "      coord.request_stop(e)\n",
        "\n",
        "    coord.request_stop()\n",
        "    coord.join(threads, stop_grace_period_secs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BUv6g7vgHB7T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_evaluate(model_type, run_once):\n",
        "  with tf.Graph().as_default() as g:\n",
        "    # Get images and labels for CIFAR-10. \n",
        "    images, labels = eval_data()\n",
        "\n",
        "    # Get loss\n",
        "    logits = model_type(images)\n",
        "    loss = get_loss(logits, labels)\n",
        "    \n",
        "    # Get predictions.\n",
        "    top_k_op = tf.nn.in_top_k(logits, labels, 1)\n",
        "\n",
        "    # Restore the moving average version of the learned variables for eval.\n",
        "    variable_averages = tf.train.ExponentialMovingAverage(0.9999)\n",
        "    variables_to_restore = variable_averages.variables_to_restore()\n",
        "    saver = tf.train.Saver(variables_to_restore)\n",
        "\n",
        "    # Build the summary operation based on the TF collection of Summaries.\n",
        "    summary_op = tf.summary.merge_all()\n",
        "\n",
        "    summary_writer = tf.summary.FileWriter(eval_dir, g)\n",
        "    \n",
        "    while True:\n",
        "      evaluate(saver, summary_writer, top_k_op, summary_op)\n",
        "      if run_once:\n",
        "           break\n",
        "        #time.sleep(FLAGS.eval_interval_secs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2MxjfAPGbss",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if tf.gfile.Exists(eval_dir):\n",
        "    tf.gfile.DeleteRecursively(eval_dir)\n",
        "tf.gfile.MakeDirs(eval_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ttjw8ic6G9fe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "run_evaluate(inference_cnn, True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}